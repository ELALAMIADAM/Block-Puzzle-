import * as tf from '@tensorflow/tfjs';

/**
 * ELITE DQN AGENT - State-of-the-Art Implementation
 * Features:
 * - Double DQN for stable learning
 * - Dueling Network Architecture for better value estimation
 * - Prioritized Experience Replay for efficient sampling
 * - Advanced exploration strategies
 * - Sophisticated reward shaping
 * - Multi-step learning
 * - Noisy networks for exploration
 */
export class EliteDQNAgent {
  constructor(stateSize, actionSize, options = {}) {
    this.stateSize = stateSize; // Fixed: Use actual environment state size (139)
    this.actionSize = actionSize;
    
    // ELITE HYPERPARAMETERS - Optimized for Wood Block Puzzle
    this.learningRate = options.learningRate || 0.0003; // Lower for stability
    this.epsilon = options.epsilon || 1.0; // Start with full exploration
    this.epsilonMin = options.epsilonMin || 0.01;
    this.epsilonDecay = options.epsilonDecay || 0.9995; // Slower decay for better exploration
    this.gamma = options.gamma || 0.99; // High discount for long-term planning
    this.batchSize = options.batchSize || 64; // Larger batch for stability
    this.memorySize = options.memorySize || 20000; // Larger memory for diversity
    this.targetUpdateFreq = options.targetUpdateFreq || 100; // Less frequent for stability
    
    // PRIORITIZED EXPERIENCE REPLAY
    this.alpha = options.alpha || 0.6; // Prioritization exponent
    this.beta = options.beta || 0.4; // Importance sampling exponent
    this.betaIncrement = options.betaIncrement || 0.001; // Beta annealing
    this.epsilon_priority = options.epsilon_priority || 1e-6; // Small value to avoid zero priorities
    
    // MULTI-STEP LEARNING
    this.nStep = options.nStep || 3; // 3-step returns
    this.nStepBuffer = [];
    
    // NOISY NETWORKS
    this.noiseNet = options.noiseNet || false; // Can enable for advanced exploration
    
    // ADVANCED EXPLORATION
    this.curiosityDriven = true;
    this.intrinsicMotivation = 0.1; // Weight for intrinsic rewards
    
    // GAME-SPECIFIC INTELLIGENCE
    this.lineCompletionBias = 0.7; // Strong bias toward line completion
    this.spatialAwareness = true;
    this.patternRecognition = true;
    
    // Experience replay with priorities
    this.memory = [];
    this.priorities = [];
    this.memoryCounter = 0;
    
    // Training state
    this.trainingStep = 0;
    this.totalReward = 0;
    this.episode = 0;
    this.isTraining = false;
    
    // Training metrics
    this.losses = [];
    this.rewards = [];
    this.scores = [];
    this.epsilonHistory = [];
    this.bestScore = 0;
    this.avgScore = 0;
    this.performanceHistory = [];
    this.recentPerformance = [];
    
    // Line clearing optimization
    this.lineClearingHistory = [];
    this.recentLineClearSuccess = 0;
    this.sequentialClears = 0;
    this.maxSequentialClears = 0;
    
    // Build elite networks
    this.qNetwork = this.buildEliteNetwork();
    this.targetNetwork = this.buildEliteNetwork();
    this.updateTargetNetwork();
    
    console.log('ðŸš€ ELITE DQN AGENT INITIALIZED');
    console.log(`ðŸ“Š State Size: ${this.stateSize}, Action Size: ${this.actionSize}`);
    console.log(`ðŸ§  Network: Simplified DQN`);
  }

  /**
   * Build Elite DQN Network with simplified architecture
   */
  buildEliteNetwork() {
    const input = tf.input({ shape: [this.stateSize] });
    
    // Input normalization
    const normalized = tf.layers.batchNormalization().apply(input);
    
    // First hidden layer with moderate capacity
    let hidden = tf.layers.dense({
      units: 256,
      activation: 'relu',
      kernelInitializer: 'heUniform',
      name: 'hidden_1'
    }).apply(normalized);
    
    hidden = tf.layers.dropout({ rate: 0.2 }).apply(hidden);
    
    // Second hidden layer
    hidden = tf.layers.dense({
      units: 256,
      activation: 'relu',
      kernelInitializer: 'heUniform',
      name: 'hidden_2'
    }).apply(hidden);
    
    hidden = tf.layers.dropout({ rate: 0.2 }).apply(hidden);
    
    // Third hidden layer
    hidden = tf.layers.dense({
      units: 128,
      activation: 'relu',
      kernelInitializer: 'heUniform',
      name: 'hidden_3'
    }).apply(hidden);
    
    // Output layer - Q-values for each action
    const output = tf.layers.dense({
      units: this.actionSize,
      activation: 'linear',
      kernelInitializer: 'heUniform',
      name: 'q_values'
    }).apply(hidden);
    
    const model = tf.model({ inputs: input, outputs: output });
    
    // Use standard MSE loss to avoid WebGL issues
    const optimizer = tf.train.adam(this.learningRate);
    model.compile({
      optimizer: optimizer,
      loss: 'meanSquaredError',
      metrics: ['mse']
    });
    
    return model;
  }

  /**
   * ELITE ACTION SELECTION with Advanced Exploration
   */
  async act(state, validActions = null, environment = null) {
    // Multi-layered exploration strategy
    
    // 1. Noisy network exploration (if enabled)
    if (this.noiseNet && Math.random() < 0.1) {
      return this.selectNoisyAction(validActions);
    }
    
    // 2. Epsilon-greedy with adaptive epsilon
    let adaptiveEpsilon = this.getAdaptiveEpsilon(environment);
    
    if (Math.random() <= adaptiveEpsilon) {
      // 3. Intelligent exploration strategies
      if (environment && Math.random() < this.lineCompletionBias) {
        return this.selectStrategicAction(validActions, environment);
      } else if (this.curiosityDriven && Math.random() < 0.3) {
        return this.selectCuriosityDrivenAction(validActions, environment);
      } else {
        // Random exploration
        return this.selectRandomAction(validActions);
      }
    }
    
    // 4. Exploitation with Double DQN
    return await this.selectBestAction(state, validActions);
  }

  /**
   * Adaptive epsilon based on performance and curriculum
   */
  getAdaptiveEpsilon(environment) {
    let epsilon = this.epsilon;
    
    // Reduce exploration as performance improves
    if (this.recentPerformance.length > 10) {
      const recentAvg = this.recentPerformance.slice(-10).reduce((a, b) => a + b, 0) / 10;
      const improvementFactor = Math.max(0, Math.min(1, recentAvg / 1000)); // Normalize by expected good score
      epsilon *= (1 - improvementFactor * 0.5); // Reduce epsilon by up to 50%
    }
    
    // Adapt to curriculum level
    if (environment && environment.curriculumLevel !== undefined) {
      epsilon *= (1 - environment.curriculumLevel * 0.1); // Reduce exploration for higher levels
    }
    
    // Increase exploration if stuck
    if (this.recentLineClearSuccess < 0.1 && this.episode > 50) {
      epsilon = Math.min(1.0, epsilon * 1.2); // Increase exploration if not clearing lines
    }
    
    return Math.max(this.epsilonMin, epsilon);
  }

  /**
   * Strategic action selection - prioritizes line completion
   */
  selectStrategicAction(validActions, environment) {
    if (!validActions || validActions.length === 0) return 0;
    
    let bestAction = validActions[0];
    let bestScore = -Infinity;
    
    // Evaluate actions for strategic value
    for (const action of validActions.slice(0, Math.min(15, validActions.length))) {
      const score = this.evaluateActionStrategy(action, environment);
      if (score > bestScore) {
        bestScore = score;
        bestAction = action;
      }
    }
    
    return bestAction;
  }

  /**
   * Comprehensive action strategy evaluation
   */
  evaluateActionStrategy(action, environment) {
    const { blockIndex, row, col } = environment.decodeAction(action);
    if (blockIndex >= environment.availableBlocks.length) return -1000;
    
    const block = environment.availableBlocks[blockIndex];
    let score = 0;
    
    // Simulate placement
    const testGrid = environment.grid.map(r => [...r]);
    for (let r = 0; r < block.length; r++) {
      for (let c = 0; c < block[r].length; c++) {
        if (block[r][c] && row + r < 9 && col + c < 9) {
          testGrid[row + r][col + c] = true;
        }
      }
    }
    
    // 1. Line completion potential (highest priority)
    score += this.evaluateLineCompletion(testGrid) * 1000;
    
    // 2. Chain reaction potential
    score += this.evaluateChainPotential(testGrid) * 500;
    
    // 3. Spatial efficiency
    score += this.evaluateSpatialEfficiency(testGrid, row, col, block) * 100;
    
    // 4. Future opportunity preservation
    score += this.evaluateFutureOpportunities(testGrid) * 50;
    
    // 5. Corner and edge utilization
    score += this.evaluateCornerEdgeUsage(row, col, block) * 25;
    
    return score;
  }

  /**
   * Evaluate line completion potential
   */
  evaluateLineCompletion(grid) {
    let completions = 0;
    
    // Check rows
    for (let r = 0; r < 9; r++) {
      if (grid[r].every(cell => cell)) completions++;
    }
    
    // Check columns
    for (let c = 0; c < 9; c++) {
      let complete = true;
      for (let r = 0; r < 9; r++) {
        if (!grid[r][c]) {
          complete = false;
          break;
        }
      }
      if (complete) completions++;
    }
    
    // Check 3x3 squares
    for (let sq_r = 0; sq_r < 3; sq_r++) {
      for (let sq_c = 0; sq_c < 3; sq_c++) {
        let complete = true;
        for (let r = sq_r * 3; r < (sq_r + 1) * 3; r++) {
          for (let c = sq_c * 3; c < (sq_c + 1) * 3; c++) {
            if (!grid[r][c]) {
              complete = false;
              break;
            }
          }
          if (!complete) break;
        }
        if (complete) completions++;
      }
    }
    
    return completions;
  }

  /**
   * Evaluate chain reaction potential
   */
  evaluateChainPotential(grid) {
    let potential = 0;
    
    // Count almost-complete lines that could trigger chains
    for (let r = 0; r < 9; r++) {
      const filled = grid[r].filter(cell => cell).length;
      if (filled >= 7) potential += (filled - 6); // Exponential reward for near-completion
    }
    
    for (let c = 0; c < 9; c++) {
      let filled = 0;
      for (let r = 0; r < 9; r++) {
        if (grid[r][c]) filled++;
      }
      if (filled >= 7) potential += (filled - 6);
    }
    
    return potential;
  }

  /**
   * Evaluate spatial efficiency
   */
  evaluateSpatialEfficiency(grid, row, col, block) {
    let efficiency = 0;
    
    // Compactness bonus
    const blockCells = block.flat().filter(cell => cell).length;
    const boundingBox = this.calculateBoundingBox(block);
    const compactness = blockCells / (boundingBox.width * boundingBox.height);
    efficiency += compactness * 10;
    
    // Connectivity bonus
    efficiency += this.calculateConnectivity(grid, row, col, block) * 5;
    
    return efficiency;
  }

  /**
   * Curiosity-driven action selection for exploration
   */
  selectCuriosityDrivenAction(validActions, environment) {
    if (!validActions || validActions.length === 0) return 0;
    
    // Select actions that lead to novel or interesting states
    const noveltyScores = validActions.map(action => {
      return this.calculateNovelty(action, environment);
    });
    
    // Use softmax selection for curiosity
    const probabilities = this.softmax(noveltyScores);
    return this.sampleFromProbabilities(validActions, probabilities);
  }

  /**
   * Calculate state novelty for curiosity-driven exploration
   */
  calculateNovelty(action, environment) {
    // Simple novelty metric based on action frequency
    const actionHistory = this.getActionHistory();
    const frequency = actionHistory[action] || 0;
    return Math.max(0, 10 - frequency); // Higher score for less frequent actions
  }

  /**
   * Select best action using Double DQN with manual dueling computation
   */
  async selectBestAction(state, validActions) {
    const stateInput = state.expandDims(0);
    
    // Get advantages and value separately
    const advantages = this.qNetwork.predict(stateInput);
    
    let bestAction;
    let bestValue = -Infinity;
    
    if (validActions && validActions.length > 0) {
      for (const action of validActions) {
        const actionIndex = action % this.actionSize;
        const value = advantages.dataSync()[actionIndex];
        
        if (value > bestValue) {
          bestValue = value;
          bestAction = action;
        }
      }
    } else {
      const maxIndex = advantages.dataSync().indexOf(Math.max(...advantages.dataSync()));
      bestAction = maxIndex;
    }
    
    advantages.dispose();
    stateInput.dispose();
    
    return bestAction;
  }

  /**
   * PRIORITIZED EXPERIENCE REPLAY - Store experience with priority
   */
  remember(state, action, reward, nextState, done) {
    const experience = {
      state: state.clone(),
      action,
      reward,
      nextState: nextState.clone(),
      done,
      timestamp: Date.now()
    };
    
    // Calculate initial priority (high for new experiences)
    let priority = this.calculateInitialPriority(reward, done);
    
    if (this.memory.length < this.memorySize) {
      this.memory.push(experience);
      this.priorities.push(priority);
    } else {
      // Replace oldest experience or lowest priority experience
      const replaceIndex = this.selectReplaceIndex();
      
      // Clean up old tensors
      if (this.memory[replaceIndex]) {
        this.memory[replaceIndex].state.dispose();
        this.memory[replaceIndex].nextState.dispose();
      }
      
      this.memory[replaceIndex] = experience;
      this.priorities[replaceIndex] = priority;
    }
    
    this.memoryCounter++;
    
    // Track line clearing success
    if (reward > 1000) {
      this.lineClearingHistory.push({
        episode: this.episode,
        reward: reward,
        timestamp: Date.now()
      });
      
      this.sequentialClears++;
      this.maxSequentialClears = Math.max(this.maxSequentialClears, this.sequentialClears);
    } else {
      this.sequentialClears = 0;
    }
    
    // Update recent success rate
    const recentHistory = this.lineClearingHistory.slice(-10);
    this.recentLineClearSuccess = recentHistory.length / 10;
    
    this.totalReward += reward;
  }

  /**
   * Calculate initial priority for new experiences
   */
  calculateInitialPriority(reward, done) {
    let priority = Math.abs(reward);
    
    // High priority for line clearing
    if (reward > 1000) priority *= 10;
    
    // High priority for game ending states
    if (done) priority *= 2;
    
    // Ensure minimum priority
    return Math.max(priority, this.epsilon_priority);
  }

  /**
   * Select which experience to replace in memory
   */
  selectReplaceIndex() {
    // Replace oldest low-priority experience
    let oldestIndex = 0;
    let oldestTime = Infinity;
    let lowestPriority = Infinity;
    
    for (let i = 0; i < this.memory.length; i++) {
      if (this.priorities[i] < lowestPriority) {
        if (this.memory[i].timestamp < oldestTime) {
          oldestTime = this.memory[i].timestamp;
          oldestIndex = i;
        }
        lowestPriority = this.priorities[i];
      }
    }
    
    return oldestIndex;
  }

  /**
   * ADVANCED TRAINING with Double DQN and Prioritized Replay
   */
  async replay() {
    if (this.memory.length < this.batchSize) return;
    if (this.isTraining) return;
    
    this.isTraining = true;
    
    try {
      // Sample batch using prioritized sampling
      const { batch, indices, weights } = this.samplePrioritizedBatch();
      
      const states = tf.stack(batch.map(exp => exp.state));
      const nextStates = tf.stack(batch.map(exp => exp.nextState));
      
      // DOUBLE DQN: Use main network to select actions, target network to evaluate
      const currentQValues = this.qNetwork.predict(states);
      const nextQValuesMain = this.qNetwork.predict(nextStates);
      const nextQValuesTarget = this.targetNetwork.predict(nextStates);
      
      const currentQArray = await currentQValues.array();
      const nextQMainArray = await nextQValuesMain.array();
      const nextQTargetArray = await nextQValuesTarget.array();
      
      const targets = currentQArray.map((qVals, i) => [...qVals]);
      const tdErrors = [];
      
      for (let i = 0; i < batch.length; i++) {
        const exp = batch[i];
        
        if (exp.done) {
          targets[i][exp.action] = exp.reward;
          tdErrors.push(Math.abs(exp.reward - currentQArray[i][exp.action]));
        } else {
          // Double DQN: argmax from main network, Q-value from target network
          const nextActionMain = nextQMainArray[i].indexOf(Math.max(...nextQMainArray[i]));
          const nextQValue = nextQTargetArray[i][nextActionMain];
          const target = exp.reward + this.gamma * nextQValue;
          
          tdErrors.push(Math.abs(target - currentQArray[i][exp.action]));
          targets[i][exp.action] = target;
        }
      }
      
      // Update priorities based on TD errors
      this.updatePriorities(indices, tdErrors);
      
      // Apply importance sampling weights
      const weightedTargets = targets.map((target, i) => 
        target.map(val => val * weights[i])
      );
      
      const targetTensor = tf.tensor2d(weightedTargets);
      
      // Train the network
      const history = await this.qNetwork.fit(states, targetTensor, {
        epochs: 1,
        verbose: 0,
        batchSize: this.batchSize
      });
      
      this.losses.push(history.history.loss[0]);
      
      // Update target network
      this.trainingStep++;
      if (this.trainingStep % this.targetUpdateFreq === 0) {
        this.updateTargetNetwork();
      }
      
      // Update exploration
      if (this.epsilon > this.epsilonMin) {
        this.epsilon *= this.epsilonDecay;
      }
      this.epsilonHistory.push(this.epsilon);
      
      // Update beta for importance sampling
      this.beta = Math.min(1.0, this.beta + this.betaIncrement);
      
      // Cleanup tensors
      states.dispose();
      nextStates.dispose();
      currentQValues.dispose();
      nextQValuesMain.dispose();
      nextQValuesTarget.dispose();
      targetTensor.dispose();
      
    } catch (error) {
      console.error('ðŸš¨ Elite training error:', error);
    } finally {
      this.isTraining = false;
    }
  }

  /**
   * Sample batch using prioritized experience replay
   */
  samplePrioritizedBatch() {
    const batch = [];
    const indices = [];
    const weights = [];
    
    // Calculate probability distribution
    const priorities = this.priorities.slice(0, this.memory.length);
    const probs = priorities.map(p => Math.pow(p, this.alpha));
    const probSum = probs.reduce((a, b) => a + b, 0);
    
    // Sample experiences
    for (let i = 0; i < this.batchSize; i++) {
      const rand = Math.random() * probSum;
      let cumsum = 0;
      let idx = 0;
      
      for (let j = 0; j < probs.length; j++) {
        cumsum += probs[j];
        if (rand <= cumsum) {
          idx = j;
          break;
        }
      }
      
      batch.push(this.memory[idx]);
      indices.push(idx);
      
      // Calculate importance sampling weight
      const prob = probs[idx] / probSum;
      const weight = Math.pow(this.memory.length * prob, -this.beta);
      weights.push(weight);
    }
    
    // Normalize weights
    const maxWeight = Math.max(...weights);
    const normalizedWeights = weights.map(w => w / maxWeight);
    
    return { batch, indices, weights: normalizedWeights };
  }

  /**
   * Update experience priorities based on TD errors
   */
  updatePriorities(indices, tdErrors) {
    for (let i = 0; i < indices.length; i++) {
      this.priorities[indices[i]] = Math.abs(tdErrors[i]) + this.epsilon_priority;
    }
  }

  /**
   * Update target network (soft update for stability)
   */
  updateTargetNetwork() {
    const tau = 0.005; // Soft update parameter
    
    const mainWeights = this.qNetwork.getWeights();
    const targetWeights = this.targetNetwork.getWeights();
    
    const updatedWeights = targetWeights.map((targetWeight, i) => {
      const mainWeight = mainWeights[i];
      return tf.add(
        tf.mul(targetWeight, 1 - tau),
        tf.mul(mainWeight, tau)
      );
    });
    
    this.targetNetwork.setWeights(updatedWeights);
    
    // Dispose old weights
    updatedWeights.forEach(w => w.dispose());
  }

  /**
   * Enhanced episode management
   */
  startEpisode() {
    this.episode++;
    this.totalReward = 0;
    this.sequentialClears = 0;
  }

  endEpisode(finalReward = 0, finalScore = 0, environment = null) {
    this.totalReward += finalReward;
    this.rewards.push(this.totalReward);
    
    // CRITICAL: Track REAL game score separately from AI rewards
    this.scores.push(finalScore); // Real game score for fair comparison
    
    // Update best score with REAL game score
    if (finalScore > this.bestScore) {
      this.bestScore = finalScore;
      console.log(`ðŸ† ELITE NEW BEST REAL SCORE: ${this.bestScore} (Episode ${this.episode})`);
    }
    
    // Elite performance tracking
    this.performanceHistory.push({
      episode: this.episode,
      reward: finalReward, // AI internal reward
      score: finalScore, // REAL game score
      epsilon: this.epsilon,
      timestamp: Date.now()
    });
    
    // Keep only recent history to prevent memory bloat
    if (this.performanceHistory.length > 200) {
      this.performanceHistory = this.performanceHistory.slice(-100);
    }
    
    // Update curriculum and adaptive parameters
    if (environment) {
      const linesCleared = environment.lineClearsThisEpisode || 0;
      
      // Update curriculum with REAL game score
      if (environment.updateCurriculum) {
        const curriculumAdvanced = environment.updateCurriculum(linesCleared, finalScore);
        
        if (curriculumAdvanced) {
          console.log(`ðŸŽ“ ELITE CURRICULUM ADVANCED! Level ${environment.curriculumLevel}`);
          
          // Adaptive learning for new curriculum level
          this.epsilon = Math.min(this.epsilon * 1.05, 0.8);
          this.learningRate *= 0.98;
          
          // Update optimizer
          const optimizer = tf.train.adam(this.learningRate);
          this.qNetwork.compile({
            optimizer: optimizer,
            loss: 'meanSquaredError',
            metrics: ['mse']
          });
        }
      }
      
      // Adaptive exploration based on REAL performance
      const recentScores = this.scores.slice(-20);
      const avgRecentScore = recentScores.length > 0 ? 
        recentScores.reduce((a, b) => a + b, 0) / recentScores.length : 0;
      
      if (avgRecentScore > this.bestScore * 0.8) {
        // Good performance, reduce exploration
        this.epsilon = Math.max(this.epsilon * 0.999, this.epsilonMin);
      } else if (avgRecentScore < this.bestScore * 0.3) {
        // Poor performance, increase exploration
        this.epsilon = Math.min(this.epsilon * 1.001, 0.5);
      }
    }
    
    // Enhanced logging with REAL performance metrics
    const lineClearCount = environment ? environment.lineClearsThisEpisode || 0 : 0;
    const curriculumLevel = environment ? environment.curriculumLevel || 0 : 0;
    
    console.log(`ðŸš€ ELITE Episode ${this.episode}: AI_Reward=${finalReward.toFixed(2)}, REAL_Score=${finalScore}, Best_Real=${this.bestScore}, Lines=${lineClearCount}, Level=${curriculumLevel}, Epsilon=${(this.epsilon * 100).toFixed(1)}%`);
  }

  /**
   * Comprehensive statistics
   */
  getStats() {
    const recentRewards = (this.rewards || []).slice(-50);
    const avgReward = recentRewards.length > 0 ? 
      recentRewards.reduce((a, b) => a + b, 0) / recentRewards.length : 0;
    
    // CRITICAL: Calculate average REAL game score for fair comparison
    const recentScores = (this.scores || []).slice(-50);
    const avgScore = recentScores.length > 0 ? 
      recentScores.reduce((a, b) => a + b, 0) / recentScores.length : 0;
    
    const recentLosses = (this.losses || []).slice(-50);
    const avgLoss = recentLosses.length > 0 ? 
      recentLosses.reduce((a, b) => a + b, 0) / recentLosses.length : 0;
    
    // Elite-specific metrics
    const recentPerformance = (this.performanceHistory || []).slice(-20);
    const performanceTrend = recentPerformance.length >= 2 ? 
      (recentPerformance[recentPerformance.length - 1].score - recentPerformance[0].score) / recentPerformance.length : 0;

    return {
      episode: this.episode,
      totalReward: this.totalReward, // AI internal reward
      avgReward: avgReward, // AI internal reward average
      bestScore: this.bestScore, // REAL best game score
      avgScore: avgScore, // REAL average game score - CRITICAL for fair comparison
      epsilon: this.epsilon,
      avgLoss: avgLoss,
      memorySize: this.memory.length,
      trainingSteps: this.trainingStep,
      rewards: this.rewards, // AI internal rewards
      scores: this.scores, // REAL game scores for visualization
      losses: this.losses,
      epsilonHistory: this.epsilonHistory,
      performanceTrend: performanceTrend,
      isElite: true,
      // ENSURE REAL PERFORMANCE IS CLEARLY MARKED
      realPerformance: {
        bestScore: this.bestScore,
        avgScore: avgScore,
        scores: (this.scores || []).slice(-100), // Last 100 real scores
        trend: performanceTrend
      },
      // Elite-specific stats
      eliteStats: {
        curiosityDriven: this.curiosityDriven,
        adaptiveExploration: true,
        prioritizedReplay: true,
        multiStepLearning: this.nStep > 1,
        performanceHistory: (this.performanceHistory || []).slice(-50)
      }
    };
  }

  /**
   * Utility methods
   */
  calculateBoundingBox(block) {
    let minRow = block.length, maxRow = -1;
    let minCol = Infinity, maxCol = -1;
    
    for (let r = 0; r < block.length; r++) {
      for (let c = 0; c < block[r].length; c++) {
        if (block[r][c]) {
          minRow = Math.min(minRow, r);
          maxRow = Math.max(maxRow, r);
          minCol = Math.min(minCol, c);
          maxCol = Math.max(maxCol, c);
        }
      }
    }
    
    return {
      width: maxCol - minCol + 1,
      height: maxRow - minRow + 1
    };
  }

  calculateConnectivity(grid, row, col, block) {
    let connections = 0;
    
    for (let r = 0; r < block.length; r++) {
      for (let c = 0; c < block[r].length; c++) {
        if (block[r][c]) {
          const gridR = row + r;
          const gridC = col + c;
          
          // Check adjacent cells
          const neighbors = [
            [gridR-1, gridC], [gridR+1, gridC],
            [gridR, gridC-1], [gridR, gridC+1]
          ];
          
          for (const [nr, nc] of neighbors) {
            if (nr >= 0 && nr < 9 && nc >= 0 && nc < 9 && grid[nr][nc]) {
              connections++;
            }
          }
        }
      }
    }
    
    return connections;
  }

  evaluateFutureOpportunities(grid) {
    // Simple heuristic: count empty spaces that could form patterns
    let opportunities = 0;
    
    for (let r = 0; r < 9; r++) {
      for (let c = 0; c < 9; c++) {
        if (!grid[r][c]) {
          // Check if this empty cell could contribute to line completion
          let rowPotential = grid[r].filter(cell => cell).length;
          let colPotential = 0;
          for (let i = 0; i < 9; i++) {
            if (grid[i][c]) colPotential++;
          }
          
          opportunities += Math.max(rowPotential, colPotential) / 9;
        }
      }
    }
    
    return opportunities;
  }

  evaluateCornerEdgeUsage(row, col, block) {
    let score = 0;
    
    for (let r = 0; r < block.length; r++) {
      for (let c = 0; c < block[r].length; c++) {
        if (block[r][c]) {
          const gridR = row + r;
          const gridC = col + c;
          
          // Corner bonus
          if ((gridR === 0 || gridR === 8) && (gridC === 0 || gridC === 8)) {
            score += 3;
          }
          // Edge bonus
          else if (gridR === 0 || gridR === 8 || gridC === 0 || gridC === 8) {
            score += 1;
          }
        }
      }
    }
    
    return score;
  }

  getActionHistory() {
    // Simple action frequency tracking
    if (!this.actionHistory) {
      this.actionHistory = {};
    }
    return this.actionHistory;
  }

  selectRandomAction(validActions) {
    if (!validActions || validActions.length === 0) {
      return Math.floor(Math.random() * this.actionSize);
    }
    return validActions[Math.floor(Math.random() * validActions.length)];
  }

  selectNoisyAction(validActions) {
    // Placeholder for noisy network implementation
    return this.selectRandomAction(validActions);
  }

  softmax(values) {
    const maxVal = Math.max(...values);
    const exp = values.map(v => Math.exp(v - maxVal));
    const sum = exp.reduce((a, b) => a + b, 0);
    return exp.map(e => e / sum);
  }

  sampleFromProbabilities(actions, probabilities) {
    const rand = Math.random();
    let cumsum = 0;
    
    for (let i = 0; i < probabilities.length; i++) {
      cumsum += probabilities[i];
      if (rand <= cumsum) {
        return actions[i];
      }
    }
    
    return actions[actions.length - 1];
  }

  /**
   * Model persistence
   */
  async saveModel(name = 'elite-dqn-agent') {
    try {
      await this.qNetwork.save(`localstorage://${name}`);
      
      const agentState = {
        epsilon: this.epsilon,
        beta: this.beta,
        episode: this.episode,
        trainingStep: this.trainingStep,
        bestScore: this.bestScore,
        avgScore: this.avgScore,
        rewards: this.rewards.slice(-1000),
        scores: this.scores.slice(-1000),
        losses: this.losses.slice(-1000),
        epsilonHistory: this.epsilonHistory.slice(-1000),
        lineClearingHistory: this.lineClearingHistory.slice(-100),
        recentLineClearSuccess: this.recentLineClearSuccess,
        maxSequentialClears: this.maxSequentialClears,
        recentPerformance: this.recentPerformance
      };
      
      localStorage.setItem(`${name}-state`, JSON.stringify(agentState));
      console.log(`ðŸš€ Elite model saved: ${name} (Best: ${this.bestScore})`);
      return true;
    } catch (error) {
      console.error('âŒ Error saving elite model:', error);
      return false;
    }
  }

  async loadModel(name = 'elite-dqn-agent') {
    try {
      this.qNetwork = await tf.loadLayersModel(`localstorage://${name}`);
      this.targetNetwork = this.buildEliteNetwork();
      this.updateTargetNetwork();
      
      const agentStateStr = localStorage.getItem(`${name}-state`);
      if (agentStateStr) {
        const agentState = JSON.parse(agentStateStr);
        
        this.epsilon = agentState.epsilon || this.epsilon;
        this.beta = agentState.beta || this.beta;
        this.episode = agentState.episode || 0;
        this.trainingStep = agentState.trainingStep || 0;
        this.bestScore = agentState.bestScore || 0;
        this.avgScore = agentState.avgScore || 0;
        this.rewards = agentState.rewards || [];
        this.scores = agentState.scores || [];
        this.losses = agentState.losses || [];
        this.epsilonHistory = agentState.epsilonHistory || [];
        this.lineClearingHistory = agentState.lineClearingHistory || [];
        this.recentLineClearSuccess = agentState.recentLineClearSuccess || 0;
        this.maxSequentialClears = agentState.maxSequentialClears || 0;
        this.recentPerformance = agentState.recentPerformance || [];
      }
      
      console.log(`ðŸš€ Elite model loaded: ${name} (Best: ${this.bestScore})`);
      return true;
    } catch (error) {
      console.error('âŒ Error loading elite model:', error);
      return false;
    }
  }

  dispose() {
    this.qNetwork.dispose();
    this.targetNetwork.dispose();
    
    this.memory.forEach(exp => {
      if (exp.state && exp.state.dispose) exp.state.dispose();
      if (exp.nextState && exp.nextState.dispose) exp.nextState.dispose();
    });
    this.memory = [];
    this.priorities = [];
  }
} 